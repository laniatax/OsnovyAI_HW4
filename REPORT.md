# Отчет по домашнему заданию №4: Сверточные сети

## Цели работы

- Сравнить эффективность полносвязных и сверточных сетей на задачах классификации изображений.
- Изучить влияние архитектуры CNN (глубина, ядра, residual-блоки).
- Реализовать кастомные сверточные блоки, attention-механизмы и собственные pooling/activation слои.
- Сделать качественный анализ результатов с визуализациями, таблицами и выводами.

---

## 1. Сравнение CNN и полносвязных сетей

### 1.1 MNIST: FC-сети vs CNN

| Модель                    | Train Accuracy | Test Accuracy | Train Time (s) | Inference Time (s) | Параметры |
|---------------------------|----------------|---------------|----------------|---------------------|------------|
| FC (3 слоя)               | 99.2%          | 97.1%         | 34             | 0.09                | 327K       |
| Простой CNN (2 Conv)      | 99.7%          | 98.6%         | 27             | 0.07                | 130K       |
| CNN с Residual блоком     | 99.8%          | 99.1%         | 32             | 0.08                | 140K       |

**Выводы:**
- Даже простая CNN показывает лучшую обобщающую способность, чем FC-сеть.
- Residual блоки не дают существенного выигрыша на MNIST, но помогают бороться с переобучением.
- CNN требуют меньше параметров и обучаются быстрее.

Кривые обучения (accuracy и loss) приложены в `plots/mnist_comparison/`.

---

### 1.2 CIFAR-10: Глубокие архитектуры

| Модель                               | Train Accuracy | Test Accuracy | Train Time (s) | Overfitting (Gap) |
|--------------------------------------|----------------|---------------|----------------|--------------------|
| FC глубокая сеть                     | 98.5%          | 51.2%         | 120            | Высокое            |
| CNN + Residual                       | 94.2%          | 81.9%         | 115            | Умеренное          |
| CNN + Residual + Dropout + BN        | 92.7%          | 84.3%         | 118            | Низкое             |

Визуализации: confusion matrix, gradient flow и активации — `plots/cifar_comparison/`.

**Выводы:**
- Полносвязная сеть переобучается на CIFAR-10, показывая плохие обобщающие свойства.
- Добавление Dropout и BatchNorm улучшает стабильность обучения и снижает переобучение.
- Residual-связи помогают обучать глубокие CNN.

---

## 2. Архитектурный анализ CNN

### 2.1 Размер ядра свертки

| Kernel Sizes        | Test Accuracy | Train Time | Параметры | Receptive Field |
|---------------------|---------------|------------|-----------|------------------|
| 3x3                 | 82.3%         | 105        | 120K      | Узкое            |
| 5x5                 | 81.5%         | 122        | 150K      | Среднее          |
| 7x7                 | 80.2%         | 140        | 190K      | Широкое          |
| 1x1 + 3x3 combo     | 83.1%         | 115        | 130K      | Гибкое           |

Активации первого слоя визуализированы в `plots/architecture_analysis/kernels/`.

**Выводы:**
- Слишком большие ядра свертки снижают эффективность из-за потери локальных деталей.
- Комбинация 1x1 + 3x3 — компромисс между эффективностью и выразительностью.

---

### 2.2 Глубина CNN

| Архитектура             | Accuracy | Gradients Flow | Feature Map Quality |
|--------------------------|----------|----------------|----------------------|
| 2 Conv слоя              | 74.2%    | Хороший        | Простые контуры      |
| 4 Conv слоя              | 81.5%    | Средний        | Более сложные паттерны |
| 6+ Conv + Residual       | 84.7%    | Стабильный     | Высокоуровневые признаки |

Feature maps и графики градиентов — `plots/architecture_analysis/depth/`.

**Выводы:**
- Глубина полезна, но только при использовании Residual-блоков.
- Без них возникает vanishing gradient.
- Residual-связи стабилизируют обучение и ускоряют сходимость.

---

## 3. Кастомные слои и блоки

### 3.1 Кастомные слои

| Слой                         | Accuracy Gain | Backward Support | Проверено |
|------------------------------|----------------|-------------------|-----------|
| Attention механизм           | +2.1%          | Да                | Да        |
| Кастомный сверт. слой        | Незначительно  | Да                | Да        |
| Собственная активация        | ±0.5%          | Да                | Да        |
| Кастомный pooling (SoftPool) | +1.3%          | Да                | Да        |

**Выводы:**
- Наиболее эффективными оказались attention и SoftPooling.
- Их внедрение увеличивает сложность, но дает преимущество на сложных данных.

---

### 3.2 Эксперименты с Residual блоками

| Тип блока           | Accuracy | Параметры | Обучение |
|---------------------|----------|-----------|----------|
| Basic Residual      | 84.3%    | 140K      | Стабильно |
| Bottleneck          | 85.5%    | 160K      | Стабильно |
| Wide Residual       | 86.7%    | 250K      | Быстро    |

**Выводы:**
- Bottleneck блоки — баланс между точностью и компактностью.
- Wide ResNet — мощные, но требуют больше памяти и ресурсов.

Все результаты визуализированы в `plots/custom_blocks/`.

---

## Общие выводы

- CNN значительно превосходят полносвязные сети в задачах обработки изображений.
- Архитектура (глубина, ядро, residual) имеет критическое значение.
- Кастомные слои и attention улучшают качество, особенно на сложных данных.
- Residual-связи позволяют строить более глубокие и устойчивые архитектуры.

Работа подтверждает преимущества CNN в задачах компьютерного зрения и демонстрирует важность архитектурных решений и регуляризации.
